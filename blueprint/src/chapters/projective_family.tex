\chapter{Projective family of the Brownian motion}
\label{chap:projective_family}


\section{Kolmogorov extension theorem}

This theorem has been formalized in the repository \href{https://github.com/RemyDegenne/kolmogorov_extension4}{kolmogorov\_extension4}.

\begin{definition}[Projective family]\label{def:IsProjectiveMeasureFamily}
  \mathlibok
  \lean{MeasureTheory.IsProjectiveMeasureFamily}
A family of measures $P$ indexed by finite sets of $T$ is projective if, for finite sets $J \subseteq I$, the projection from $E^I$ to $E^J$ maps $P_I$ to $P_J$.
\end{definition}


\begin{definition}[Projective limit]\label{def:IsProjectiveLimit}
  \uses{def:IsProjectiveMeasureFamily}
  \mathlibok
  \lean{MeasureTheory.IsProjectiveLimit}
A measure $\mu$ on $E^T$ is the projective limit of a projective family of measures $P$ indexed by finite sets of $T$ if, for every finite set $I \subseteq T$, the projection from $E^T$ to $E^I$ maps $\mu$ to $P_I$.
\end{definition}


\begin{theorem}[Kolmogorov extension theorem]\label{thm:kolmogorovExtension}
  \uses{def:IsProjectiveLimit, def:IsProjectiveMeasureFamily}
  \leanok
  \lean{MeasureTheory.projectiveLimit, MeasureTheory.IsProjectiveLimit.unique, MeasureTheory.isProjectiveLimit_projectiveLimit, MeasureTheory.isFiniteMeasure_projectiveLimit, MeasureTheory.isProbabilityMeasure_projectiveLimit}
Let $\mathcal{X}$ be a Polish space, equipped with the Borel $\sigma$-algebra, and let $T$ be an index set.
Let $P$ be a projective family of finite measures on $\mathcal{X}$.
Then the projective limit $\mu$ of $P$ exists, is unique, and is a finite measure on $\mathcal{X}^T$.
Moreover, if $P_I$ is a probability measure for every finite set $I \subseteq T$, then $\mu$ is a probability measure.
\end{theorem}

\begin{proof}\leanok

\end{proof}


\section{Projective family of Gaussian measures}

We build a projective family of Gaussian measures indexed by $\mathbb{R}_+$.
In order to do so, we need to define specific Gaussian measures on finite index sets $\{t_1, \ldots, t_n\}$.
We want to build a multivariate Gaussian measure on $\mathbb{R}^n$ with mean $0$ and covariance matrix $C_{ij} = \min(t_i, t_j)$ for $1 \leq i,j \leq n$.

% \paragraph{First method: Gaussian increments}

% In this method, we build the Gaussian measure by adding independent Gaussian increments.

% \begin{definition}(Gaussian increment)\label{def:gaussianIncrement}
% For $v \ge 0$, the map from $\mathbb{R}$ to the probability measures on $\mathbb{R}$ defined by $x \mapsto \mathcal{N}(x, v)$ is a Markov kernel.
% We call that kernel the \emph{Gaussian increment} with variance $v$ and denote it by $\kappa^G_v$.
% \end{definition}

% TODO: perhaps the equality $\mathcal{N}(x, v) = \delta_x \ast \mathcal{N}(0, v)$ is useful to show that it is a kernel?

% \begin{definition}\label{def:gaussianFromIncrements}
%   \uses{def:gaussianIncrement}
% Let $0 \le t_1 \le \ldots \le t_n$ be non-negative reals.
% Let $\mu_0$ be the real Gaussian distribution $\mathcal{N}(0, t_1)$.
% For $i \in \{1, \ldots, n-1\}$, let $\kappa_i$ be the Markov kernel from $\mathbb{R}$ to $\mathbb{R}$ defined by $\kappa_i(x) = \mathcal{N}(x, t_{i+1} - t_i)$ (the Gaussian increment $\kappa^G_{t_{i+1} - t_i}$).
% Let $P_{t_1, \ldots, t_n}$ be the measure on $\mathbb{R}^n$ defined by $\mu_0 \otimes \kappa_1 \otimes \ldots \otimes \kappa_{n-1}$.
% \end{definition}

% TODO: explain the notation $\otimes$ in the lemma above: $\kappa_{n-1}$ takes the value at $n-1$ only to produce the distribution at $n$.

% \begin{lemma}\label{lem:isGaussian_gaussianFromIncrements}
%   \uses{def:gaussianFromIncrements, def:IsGaussian}
% $P_{t_1, \ldots, t_n}$ is a Gaussian measure on $\mathbb{R}^n$ with mean $0$ and covariance matrix $C_{ij} = \min(t_i, t_j)$ for $1 \leq i,j \leq n$.
% \end{lemma}

% \begin{proof}

% \end{proof}


% \paragraph{Second method: covariance matrix}

We prove that the matrix $C_{ij} = \min(t_i, t_j)$ is positive semidefinite, which means that there exists a Gaussian distribution with mean 0 and covariance matrix $C$.

\begin{definition}[Gram matrix]\label{def:gramMatrix}
Let $v_1, \ldots, v_n$ be vectors in an inner product space $E$.
The Gram matrix of $v_1, \ldots, v_n$ is the matrix in $\mathbb{R}^{n \times n}$ with entries $G_{ij} = \langle v_i, v_j \rangle$ for $1 \leq i,j \leq n$.
\end{definition}


\begin{lemma}\label{lem:posSemidef_gramMatrix}
  \uses{def:gramMatrix}
A gram matrix is positive semidefinite.
\end{lemma}

\begin{proof}
Symmetry is obvious from the definition.
Let $x \in E$. Then
\begin{align*}
  \langle x, G x \rangle
  &= \sum_{i,j} x_i x_j \langle v_i, v_j \rangle
  \\
  &= \langle \sum_i x_i v_i, \sum_j x_j v_j \rangle
  \\
  &= \left\Vert \sum_i x_i v_i \right\Vert^2
  \\
  &\ge 0
  \: .
\end{align*}
\end{proof}


\begin{lemma}\label{lem:C_eq_gramMatrix}
  \uses{def:gramMatrix}
Let $I = \{t_1, \ldots, t_n\}$ be a finite subset of $\mathbb{R}_+$.
For $i \le n$, let $v_i = \mathbb{I}_{[0, t_i]}$ be the indicator function of the interval $[0, t_i]$, as an element of $L^2(\mathbb{R})$.
Then the Gram matrix of $v_1, \ldots, v_n$ is equal to the matrix $C_{ij} = \min(t_i, t_j)$ for $1 \leq i,j \leq n$.
\end{lemma}

\begin{proof}
By definition of the inner product in $L^2(\mathbb{R})$,
\begin{align*}
  \langle v_i, v_j \rangle
  &= \int_{\mathbb{R}} \mathbb{I}_{[0, t_i]}(x) \mathbb{I}_{[0, t_j]}(x) \: dx
  = \min(t_i, t_j)
  \: .
\end{align*}
\end{proof}


\begin{lemma}\label{lem:posSemidef_brownianCov}
For $I = \{t_1, \ldots, t_n\}$ a finite subset of $\mathbb{R}_+$, let $C \in \mathbb{R}^{n \times n}$ be the matrix $C_{ij} = \min(t_i, t_j)$ for $1 \leq i,j \leq n$.
Then $C$ is positive semidefinite.
\end{lemma}

\begin{proof}\uses{lem:C_eq_gramMatrix, lem:posSemidef_gramMatrix}
$C$ is a Gram matrix by Lemma~\ref{lem:C_eq_gramMatrix}.
By Lemma~\ref{lem:posSemidef_gramMatrix}, it is positive semidefinite.
\end{proof}


\paragraph{Definition of the projective family and extension}

\begin{definition}[Projective family of the Brownian motion]\label{def:gaussianProjectiveFamily}
  \uses{def:multivariateGaussian, lem:posSemidef_brownianCov}
For $I = \{t_1, \ldots, t_n\}$ a finite subset of $\mathbb{R}_+$, let $P^B_I$ be the multivariate Gaussian measure on $\mathbb{R}^n$ with mean $0$ and covariance matrix $C_{ij} = \min(t_i, t_j)$ for $1 \leq i,j \leq n$.
We call the family of measures $P^B_I$ the \emph{projective family of the Brownian motion}.
\end{definition}


\begin{lemma}\label{lem:isProjectiveMeasureFamily_gaussianProjectiveFamily}
  \uses{def:gaussianProjectiveFamily, def:IsProjectiveMeasureFamily}
The projective family of the Brownian motion is a projective family of measures.
\end{lemma}

\begin{proof}
  \uses{lem:isGaussian_map, lem:isGaussian_multivariateGaussian, lem:covMatrix_map}
Let $J \subseteq I$ be finite subsets of $\mathbb{R}_+$.
We need to show that the restriction from $\mathbb{R}^I$ to $\mathbb{R}^J$ (denote it by $\pi_{IJ}$) maps $P^B_I$ to $P^B_J$.

The restriction is a continuous linear map from $\mathbb{R}^I$ to $\mathbb{R}^J$.
The map of a Gaussian measure by a continuous linear map is Gaussian (Lemma~\ref{lem:isGaussian_map}).
It thus suffices to show that the mean and covariance matrix of the map are equal to the ones of $P^B_J$.

The mean of the map is $0$, since the mean of $P^B_I$ is $0$ and the map is linear.

For the covariance matrix and $i, j \in J$, by Lemma~\ref{lem:covMatrix_map} we have
\begin{align*}
  \langle e_i, \Sigma_{\pi_{IJ*}\mu} e_j\rangle
  &= \langle \pi_{IJ}^\dagger(e_i), \Sigma_\mu \pi_{IJ}^\dagger(e_j)\rangle
  \: .
\end{align*}
$\pi_{IJ}^\dagger(u)$ is the vector of $\mathbb{R}^I$ with coordinates $(\pi_{IJ}^\dagger(u))_i = u_i$ if $i \in J$ and $(\pi_{IJ}^\dagger(u))_i = 0$ otherwise.
This gives the same covariance matrix as the one of $P^B_J$.
\end{proof}


\begin{definition}\label{def:gaussianLimit}
  \uses{thm:kolmogorovExtension, lem:isProjectiveMeasureFamily_gaussianProjectiveFamily}
We denote by TODO the projective limit of the projective family of the Brownian motion given by Theorem~\ref{thm:kolmogorovExtension}.
This is a probability measure on $\mathbb{R}^{\mathbb{R}_+}$.
\end{definition}


% \begin{definition}\label{def:}
% Let $\Omega = \mathbb{R}^{\mathbb{R}_+}$ and consider the probability space $(\Omega, TODO)$.
% The identity on that space is a function $\Omega \to \mathbb{R}_+ \to \mathbb{R}$.
% We can reorder the arguments to define a process $X : \mathbb{R}_+ \to \Omega \to \mathbb{R}$.
% That process is a Gaussian process with covariance function $C(t,s) = \min(t,s)$.
% \end{definition}
